---
title: "fmin: R and C++ implementation of Quasi-Newton multivariate optimiser"
author: "Richard Glennie"
date: "`r Sys.Date()`"
output: html_document
bibliography: refs.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r}
library(fmin)
library(ggplot2)
```

This package lets you minimize functions with multivariate inputs. It uses a quasi-Newton
algorithm described in @nocedal2006. It is similar to the popular `nlm()` function in `R`, but is implemented fully in `R`. There is also a full `C++` implementation included so that you can optimize functions within `C++` directly: this may be useful for those who implement their functions in `Rcpp`. 

This vignette shows the simple capabilities of the package. 

# Basic Usage

Let's use Rosenbrock's banana function in 2D as an example. The banana function with $2$ input variables $(x_1, x_2)$ has the form 

$$f(\textbf{x}) = (1 - x_1)^2 + 100(x_2 - x_1^2)^2$$

It is known that this function has a minimum at $(1, 1)$. 

Let's define the function and plot it. 

```{r, fig.align="center"}
# define function
banana <- function(x) { 
  return((1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2)  
}

# plot it 
gr1 <- seq(-2, 2, 0.01)
gr2 <- seq(-1, 3, 0.01)
gr <- expand.grid(gr1, gr2)
colnames(gr) <- c("x1", "x2")
gr$f <- apply(gr, 1, FUN = banana)
ggplot(gr) + 
  geom_contour_filled(aes(x = x1, y = x2, z = f),
                      breaks = c(0, 50, 100, 500, 1000, 2500, 3000)) + 
  scale_color_viridis_c()
```

We can see it is a difficult function to find the global minimum of because of the
long valley that looks like a banana. 

Let's use `fmin` from this package to optimise the function. To do that we must specify
a point to start at. I'll pretend I don't know $(1,1)$ is the best point and from the plot
guess that $(0, 0)$ could be the minimum. 

```{r}
start <- c(0, 0)
opt <- fmin(banana, start)
```

You can now access the optimisation results: 

```{r}
opt
```
The `estimate` is the input variables identified as the where the minimum of the function
occurs. The value of the function at the minimum is given by `value`. Other function information given is `g` (the gradient at the minimum) and `H` (the Hessian at the minimum). You'd expect the gradient to be very small at the minimum (as it is a stationary point). An important output is `conv` which is `TRUE` if the algorithm is likely to have converged to a stationary point, when this is `FALSE` you need to investigate why the algortihm has failed. Finally, `niter` is the number of iterations of the algorithm used. 

For comparison, we see that in this case `fmin` outperforms `nlm` (marginally): 
```{r}
nlm_opt <- nlm(banana, start)
nlm_opt
```

See [examples](https://github.com/r-glennie/fmin/blob/master/inst/examples.R) for a R script
with more examples of tricky functions to optimize using `fmin`. 

If you want to see some output as `fmin` works, then use `verbose = TRUE`: 

```{r}
opt <- fmin(banana, start, verbose = TRUE)
```

# Checking convergence 

I have already said that `conv` reports whether or not convergence is likely to have occured with
the `fmin` algorithm. Nothing in life is certain and you might want to have a closer look. Or if 
`conv` is `FALSE` you might want to see why. 

One way to do this is to plot how the gradient and function values changed as the algorithm
unfolded. To do this, you need to save the key information as the algortihm runs. 

```{r}
opt <- fmin(banana, start, save = TRUE)
```

Then you can use an function within this package to plot some useful graphs: 

```{r}
check_fmin(opt)
```
Maybe you will spot something suspicious or concerning in these graphs that can
help optimize the function or pick better starting values. 

# More advanced uses

The `fmin` function has a number of arguments that I haven't used yet. Let me describe
why you might want to use them. 

- `gobj` allows you to specify a gradient function if you know it for your problem. Otherwise, the gradient is approximated numerically using `numDeriv`. 

- `funit` The algorithm cannot work brilliantly for _any_ scale of function value, e.g. functions that go into the thousands or millions might not work very well due to computer arithmetic being weird. It is then better practise to scale your function to vary mostly between $-1$ and $1$. For the banana case, I case see the function goes up to at least $1000$, so I might select $funit = 1000$. 

- `units` Same reason for using `funit` but for the input variables. If the input variables have widely different scales (one is thousandths and the other in millions) then you can specify a scaling so that they all vary evenly between $-1$ and $1$. This is a vector with a scaling unit for each input variable.
 
- `tol` A tolerance is used to determine when convergence has occurred. You could relax this if you want to see how marginal the decision is whether convergence has occurred or not. 

- `stepmax` Every step of the algorithm is somewhere between a full Newton step (as determined by Newton's algorithm) and something shorter or longer. A `stepmax` of $1$ limits steps to be at most a Newton step or shorter. If you make `stepmax` $> 1$ then the algorithm will consider steps longer than a Newton update and `stepmax` $< 1$ means it will only consider shorter steps. 

- `maxsubsteps` Before taking a step, the algrotihm tries lots of substeps to try to find the best step to make, you can limit how many substeps it tries.

# Using fmin in C++ 

Sometimes you might have a function written in `C++` (perhaps it receives information
from `R` using `Rcpp`). You might want to optimise the function in `C++` without using an optimiser in `R`. This can reduce some overhead caused by `Rcpp` passing information between
`R` and `C++` during the optimisation. I will assume the reader is somewhat familiar with `C++`. 

To do this, you need to define your `C++` function in a particular way. Let's do it
for the banana function. 

```
#include <RcppEigen.h>
#include <iostream>
#include <fmin.h>
// [[Rcpp::depends(RcppEigen)]]

using namespace Rcpp;

// Define a class with an () operator 
class myFun {
public:
  myFun() {}
  double operator()(const Eigen::VectorXd& x) const;
};

// Define function I want to minimise 
double myFun::operator()(const Eigen::VectorXd& x) const {
  double f = 0; 
  f += (1 - x[0]) * (1 - x[0]); 
  f += 100 * (x[1] - x[0] * x[0]); 
  return f;
}
```

The key idea is to create a `C++` class where the parenthesis is an overloaded operator. 
Someone unfamilar with these `C++` concepts, can just replace the part the function
is defined above with their own function. For those familar with classes, recall that it is
possbile to store auxiliary data as data members of the class. 

You can also copy the function below that calls the `C++` version of `fmin` on 
the function defined in `myFun::operator()`: 

```
// [[Rcpp::export]]
List doF(Eigen::VectorXd x,
         int maxit = 1000,
         double tol = 1e-10,
         double stepmax = 1, 
         int maxsubsteps = 10,
         bool verbose = false,
         int digits = 4) {
         
  // create an instance of my function 
  myFun F;
  // create instance of the fmin engine 
  Fmin<myFun> fmin(F, x, maxit, tol, stepmax, maxsubsteps, verbose, digits);
  // run fmin on my function to find minimum 
  fmin.Run();
  // package up results to return to R
  List res = List::create(Named("par") = fmin.Par(),
                          Named("value") = F(fmin.Par()),
                          Named("g") = fmin.ComputeG(fmin.Par()),
                          Named("H") = fmin.ComputeH(fmin.Par()),
                          Named("conv") = fmin.Conv(),
                          Named("niter") = fmin.Iter());
  return res;
}
```

Suppose I put all the above `C++` code into a file called `banana.cpp`. I 
can now compile this code in `R` and optimise my function. 

```{r}
# Need RcppEigen and Rcpp to compile this code 
library(RcppEigen)
library(Rcpp)
# compile function
sourceCpp("banana.cpp")
```

Now I can run the optimisation in `C++` and look at the results in `R`: 
```{r}
# Run optimization
res <- doF(c(0, 0))

# Ouputs
res
```

# References

